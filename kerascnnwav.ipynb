{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional, CuDNNGRU, CuDNNLSTM, RepeatVector, RepeatVector, concatenate,ConvLSTM2D\n",
    "from keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Flatten, Convolution1D,TimeDistributed,Lambda, Activation, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.engine.topology import Layer\n",
    "from keras.initializers import Ones, Zeros\n",
    "\n",
    "from tsfresh.examples import load_robot_execution_failures\n",
    "from tsfresh import extract_features, select_features\n",
    "import optuna\n",
    "\n",
    "from common import EP\n",
    "from models import *\n",
    "from dfdb import DFDB\n",
    "\n",
    "import types\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\";  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "   div#notebook-container    { width: 95%; }\n",
       "   div#menubar-container     { width: 65%; }\n",
       "   div#maintoolbar-container { width: 99%; }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "   div#notebook-container    { width: 95%; }\n",
    "   div#menubar-container     { width: 65%; }\n",
    "   div#maintoolbar-container { width: 99%; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_lineplot(dflist, plot_features=None, n_col=3):\n",
    "\n",
    "    n_chart = len(dflist)\n",
    "    n_row = int(n_chart/n_col) if n_chart % n_col == 0 else n_row+1\n",
    "        \n",
    "    fig = plt.figure(figsize=(5*n_col, 3*n_row))\n",
    "    for i, df in enumerate(dflist):\n",
    "        ax = fig.add_subplot(n_row, n_col, i+1)\n",
    "        if type(plot_features) == type(None):\n",
    "            plot_features = df.columns.tolist()\n",
    "        for feat in plot_features:\n",
    "            sns.lineplot(x=df.index, y=df[feat], ax=ax)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train = pd.read_pickle('../feats/sample.pkl')\n",
    "df_wav = pd.read_csv('../data/train.csv', dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32})\n",
    "wav = df_wav['acoustic_data'].values\n",
    "ttf = df_wav['time_to_failure'].values\n",
    "wav_mean = df_wav['acoustic_data'].mean()\n",
    "wav_std = df_wav['acoustic_data'].std()\n",
    "# df_X_train['X'] = df_X_train['index'].apply(lambda x: np.expand_dims(wav[x:x+150_000], 1))\n",
    "# del df_wav\n",
    "# del wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train['label'] = df_X_train['y'].apply(lambda x:  int(x) if x<15 else 15)\n",
    "group = df_X_train['season'].values\n",
    "group[np.where(group==17)[0]] = 1\n",
    "df_X_train['group'] = group\n",
    "df_X_train = df_X_train.drop(columns=['season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = '../data/test/'\n",
    "X_test = []\n",
    "for f in sorted(os.listdir(test_file_path)):\n",
    "    df_test_i = pd.read_csv(test_file_path+f)\n",
    "    X_test.append({'index':f.replace('.csv',''), 'X':df_test_i['acoustic_data'].values})\n",
    "df_X_test = pd.DataFrame(X_test)\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = DFDB('../trial2/kerascnnwav.pkl', auto_commit=False)\n",
    "db.select().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from dataloader import TokenList, pad_to_longest\n",
    "    # for transformer\n",
    "except: pass\n",
    "\n",
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, eps=1e-6, **kwargs):\n",
    "        self.eps = eps\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "                                     initializer=Ones(), trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "                                    initializer=Zeros(), trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        mean = K.mean(x, axis=-1, keepdims=True)\n",
    "        std = K.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class ScaledDotProductAttention():\n",
    "    def __init__(self, d_model, attn_dropout=0.1):\n",
    "        self.temper = np.sqrt(d_model)\n",
    "        self.dropout = Dropout(attn_dropout)\n",
    "    def __call__(self, q, k, v, mask):\n",
    "        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
    "        if mask is not None:\n",
    "            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\n",
    "            attn = Add()([attn, mmask])\n",
    "        attn = Activation('softmax')(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n",
    "        return output, attn\n",
    "\n",
    "class MultiHeadAttention():\n",
    "    # mode 0 - big martixes, faster; mode 1 - more clear implementation\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n",
    "        self.mode = mode\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.dropout = dropout\n",
    "        if mode == 0:\n",
    "            self.qs_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.ks_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.vs_layer = Dense(n_head*d_v, use_bias=False)\n",
    "        elif mode == 1:\n",
    "            self.qs_layers = []\n",
    "            self.ks_layers = []\n",
    "            self.vs_layers = []\n",
    "            for _ in range(n_head):\n",
    "                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n",
    "        self.attention = ScaledDotProductAttention(d_model)\n",
    "        self.layer_norm = LayerNormalization() if use_norm else None\n",
    "        self.w_o = TimeDistributed(Dense(d_model))\n",
    "\n",
    "    def __call__(self, q, k, v, mask=None):\n",
    "        d_k, d_v = self.d_k, self.d_v\n",
    "        n_head = self.n_head\n",
    "\n",
    "        if self.mode == 0:\n",
    "            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n",
    "            ks = self.ks_layer(k)\n",
    "            vs = self.vs_layer(v)\n",
    "\n",
    "            def reshape1(x):\n",
    "                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n",
    "                x = tf.reshape(x, [s[0], s[1], n_head, d_k])\n",
    "                x = tf.transpose(x, [2, 0, 1, 3])  \n",
    "                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]\n",
    "                return x\n",
    "            qs = Lambda(reshape1)(qs)\n",
    "            ks = Lambda(reshape1)(ks)\n",
    "            vs = Lambda(reshape1)(vs)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\n",
    "            head, attn = self.attention(qs, ks, vs, mask=mask)  \n",
    "                \n",
    "            def reshape2(x):\n",
    "                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n",
    "                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) \n",
    "                x = tf.transpose(x, [1, 2, 0, 3])\n",
    "                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]\n",
    "                return x\n",
    "            head = Lambda(reshape2)(head)\n",
    "        elif self.mode == 1:\n",
    "            heads = []; attns = []\n",
    "            for i in range(n_head):\n",
    "                qs = self.qs_layers[i](q)   \n",
    "                ks = self.ks_layers[i](k) \n",
    "                vs = self.vs_layers[i](v) \n",
    "                head, attn = self.attention(qs, ks, vs, mask)\n",
    "                heads.append(head); attns.append(attn)\n",
    "            head = Concatenate()(heads) if n_head > 1 else heads[0]\n",
    "            attn = Concatenate()(attns) if n_head > 1 else attns[0]\n",
    "\n",
    "        outputs = self.w_o(head)\n",
    "        outputs = Dropout(self.dropout)(outputs)\n",
    "        if not self.layer_norm: return outputs, attn\n",
    "        # outputs = Add()([outputs, q]) # sl: fix\n",
    "        return self.layer_norm(outputs), attn\n",
    "    \n",
    "class ReshapeStandardScaler(object):\n",
    "    \n",
    "    def  __init__(self, shape, mean, std):\n",
    "        \n",
    "        assert shape[-1] == len(std.shape), 'the shape is not matched'\n",
    "        assert shape[-1] == len(mean.shape), 'the shape is not matched'\n",
    "        self.shape = shape\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        return\n",
    "    \n",
    "    def fit(self, **kwargs):\n",
    "        return\n",
    "    \n",
    "    def transform(self, X):\n",
    "        original_shape = X.shape\n",
    "        X = (X.reshape(self.shape) - self.mean)/self.std\n",
    "        return X.reshape(original_shape)\n",
    "    \n",
    "def create_path(base_dir, param):\n",
    "    if base_dir == None:\n",
    "        return None\n",
    "    fold_path = base_dir + '/' + ','.join(\"{!s}={!r}\".format(key,val) for (key,val) in param.items())\n",
    "    if not os.path.exists(fold_path):\n",
    "        os.makedirs(fold_path)\n",
    "    return fold_path\n",
    "\n",
    "# class ReshapeStandardScaler(object):\n",
    "    \n",
    "#     def  __init__(self, shape, mean, std):\n",
    "        \n",
    "#         assert shape[-1] == len(std.shape), 'the shape is not matched'\n",
    "#         assert shape[-1] == len(mean.shape), 'the shape is not matched'\n",
    "#         self.shape = shape\n",
    "#         self.std = std\n",
    "#         self.mean = mean\n",
    "#         return\n",
    "    \n",
    "#     def fit(self, **kwargs):\n",
    "#         return\n",
    "    \n",
    "#     def transform(self, X):\n",
    "#         original_shape = X.shape\n",
    "#         X = (X.reshape(self.shape) - self.mean)/self.std\n",
    "#         return X.reshape(original_shape)\n",
    "\n",
    "class Keras1DCnnRegressor(object):\n",
    "    \n",
    "    def __init__(self, batch, timesteps, input_dim, cnn_layer_sizes, cnn_kernel_size, cnn_strides, cnn_activation, \n",
    "                    fc_layer_sizes, fc_activation, dropout, solver, metric, lr, sgd_momentum, sgd_decay, base_save_dir, alias, \n",
    "                 attention_n_head=5, attention_d_model=256, attention_d_k=64, attention_d_v=64, bilstm_layer_sizes=[]):\n",
    "        \n",
    "        self.batch = batch\n",
    "        self.timesteps = timesteps\n",
    "        self.input_dim = input_dim\n",
    "        self.cnn_layer_sizes = cnn_layer_sizes\n",
    "        self.cnn_kernel_size = cnn_kernel_size\n",
    "        self.cnn_strides = cnn_strides\n",
    "        self.cnn_activation = cnn_activation\n",
    "        self.fc_layer_sizes = fc_layer_sizes\n",
    "        self.fc_activation = fc_activation\n",
    "        self.dropout = dropout\n",
    "        self.solver = solver\n",
    "        self.metric = metric\n",
    "        self.lr = lr\n",
    "        self.sgd_momentum = sgd_momentum\n",
    "        self.sgd_decay = sgd_decay\n",
    "        \n",
    "        self.regressor = self.build_graph(timesteps, input_dim, cnn_layer_sizes, cnn_kernel_size, cnn_strides, cnn_activation, \n",
    "                    fc_layer_sizes, fc_activation, attention_n_head, attention_d_model, attention_d_k, attention_d_v, bilstm_layer_sizes, dropout)\n",
    "        self.compile_graph(self.regressor, solver, metric, lr, sgd_momentum, sgd_decay)\n",
    "        \n",
    "        self.alias = alias\n",
    "        self.base_save_dir = base_save_dir\n",
    "        if (self.alias==None) & (self.base_save_dir==None):\n",
    "            self.chkpt = None\n",
    "        else:\n",
    "            self.chkpt = os.path.join(base_save_dir,'{}.hdf5'.format(alias))\n",
    "\n",
    "        return\n",
    "    \n",
    "    def build_graph(self, timesteps, input_dim, cnn_layer_sizes, cnn_kernel_size, cnn_strides, cnn_activation, \n",
    "                    fc_layer_sizes, fc_activation, attention_n_head, attention_d_model, attention_d_k, attention_d_v, bilstm_layer_sizes, dropout):\n",
    "        \n",
    "        i = Input(shape = (timesteps, input_dim))\n",
    "        x = Convolution1D( cnn_layer_sizes[0], kernel_size = cnn_kernel_size, strides = cnn_strides, activation=cnn_activation)(i)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        for units in cnn_layer_sizes[1:]:\n",
    "            x = Convolution1D(units, kernel_size = cnn_kernel_size, strides = cnn_strides, activation=cnn_activation)(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(dropout)(x)\n",
    "        for units in bilstm_layer_sizes:\n",
    "            x = Bidirectional(CuDNNLSTM(units, return_sequences=True))(x)\n",
    "        x, slf_attn = MultiHeadAttention(n_head=attention_n_head, d_model=attention_d_model, d_k=attention_d_k, d_v=attention_d_v, dropout=dropout)(x, x, x)\n",
    "        avg_pool = GlobalAveragePooling1D()(x)\n",
    "        max_pool = GlobalMaxPooling1D()(x)\n",
    "        x = concatenate([avg_pool, max_pool])\n",
    "        for units in fc_layer_sizes[:-1]:\n",
    "            x = Dense(units, activation=fc_activation)(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(dropout)(x)\n",
    "        x = Dense(fc_layer_sizes[-1], activation=fc_activation)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        y = Dense(1)(x)\n",
    "        regressor = Model(inputs = [i], outputs = [y])\n",
    "        return regressor\n",
    "    \n",
    "    def compile_graph(self, model, solver, metric, lr, momentum, decay):\n",
    "        if solver=='adam':\n",
    "            optimizer = optimizers.adam(lr=lr)\n",
    "        elif solver=='sgd':\n",
    "            optimizer = optimizers.SGD(lr=lr, decay=decay, momentum=momentum, nesterov=True)\n",
    "        model.compile(optimizer=optimizer, loss=metric)\n",
    "        return\n",
    "    \n",
    "    def fit_generator(self, train_gen, eval_set, verbose=1, epochs=200):\n",
    "        \n",
    "        \n",
    "        df_train_his = pd.DataFrame()\n",
    "#         prev_val_loss = 999999\n",
    "        for i in np.arange(epochs):\n",
    "            if type(eval_set)==type(None):\n",
    "                validation_data = None\n",
    "            else:\n",
    "                validation_data = eval_set[0]\n",
    "            his_train = self.regressor.fit_generator( generator =  train_gen,  epochs = 1,  verbose = 0,  validation_data = validation_data, callbacks = [])\n",
    "            df_train_his_i = pd.DataFrame(his_train.history)\n",
    "            df_train_his_i['epochs'] = i\n",
    "            df_train_his = pd.concat([df_train_his, df_train_his_i], axis=0)\n",
    "            \n",
    "            if verbose > 0:\n",
    "                if validation_data == None:\n",
    "                    print(df_train_his_i.epochs.values, df_train_his_i.loss.values)\n",
    "                else:\n",
    "                    print(df_train_his_i.epochs.values, df_train_his_i.loss.values, df_train_his_i.val_loss.values)\n",
    "                \n",
    "#             if (df_train_his_i.val_loss.values[0] < prev_val_loss) & (self.chkpt!=None) :\n",
    "#                 prev_val_loss = df_train_his_i.val_loss.values[0]\n",
    "#                 self.regressor.save_weights(self.chkpt)\n",
    "        \n",
    "        df_train_his.to_csv(self.base_save_dir + '/train_his.csv', index=True)\n",
    "        return\n",
    "    \n",
    "    def fit(self, X_train, y_train, eval_set, verbose=1, epochs=200):\n",
    "              \n",
    "        df_train_his = pd.DataFrame()\n",
    "#         prev_val_loss = 999999\n",
    "        for i in np.arange(epochs):\n",
    "            if type(eval_set)==type(None):\n",
    "                validation_data = None\n",
    "            else:\n",
    "                validation_data = eval_set[0]\n",
    "                assert type(eval_set[0])==tuple, 'validation_data[0] is not a tuple'\n",
    "            his_train = self.regressor.fit( X_train, y_train, epochs = 1,  verbose = 0,  batch_size = self.batch,  validation_data = validation_data,  callbacks = [])\n",
    "            df_train_his_i = pd.DataFrame(his_train.history)\n",
    "            df_train_his_i['epochs'] = i\n",
    "            df_train_his = pd.concat([df_train_his, df_train_his_i], axis=0)\n",
    "            \n",
    "            if verbose > 0:\n",
    "                if validation_data == None:\n",
    "                    print(df_train_his_i.epochs.values, df_train_his_i.loss.values)\n",
    "                else:\n",
    "                    print(df_train_his_i.epochs.values, df_train_his_i.loss.values, df_train_his_i.val_loss.values)\n",
    "                \n",
    "#             if (df_train_his_i.val_loss.values[0] < prev_val_loss) & (self.chkpt!=None) :\n",
    "#                 prev_val_loss = df_train_his_i.val_loss.values[0]\n",
    "#                 self.regressor.save_weights(self.chkpt)\n",
    "                \n",
    "        df_train_his.to_csv(self.base_save_dir + '/train_his.csv', index=True)\n",
    "            \n",
    "        return df_train_his\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.regressor.predict(X)[:,0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, x, y, x_mean, x_std, start_indexes, ts_length, batch_size, steps_per_epoch, shaking=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.start_indexes = start_indexes\n",
    "        self.ts_length = ts_length\n",
    "        self.batch_size = batch_size\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.x_mean = x_mean\n",
    "        self.x_std = x_std\n",
    "        self.shaking = shaking\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.steps_per_epoch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        start_indexes_epoch = np.random.choice(self.start_indexes, size=self.batch_size)\n",
    "        if self.shaking:\n",
    "            shifts = np.random.randint(0, int(self.ts_length*.2), size=self.batch_size) - int(self.ts_length*.1)\n",
    "        else:\n",
    "            shifts = np.zeros(self.batch_size)\n",
    "            \n",
    "        x_batch = np.empty((self.batch_size, self.ts_length))\n",
    "        y_batch = np.empty(self.batch_size, )\n",
    "\n",
    "        for i, start_idx in enumerate(start_indexes_epoch):\n",
    "            end = start_idx + shifts[i] + self.ts_length\n",
    "            if end < self.ts_length:\n",
    "                end = self.ts_length\n",
    "            if end >= self.x.shape[0]:\n",
    "                end = self.x.shape[0]\n",
    "            x_i = self.x[end-self.ts_length:end]\n",
    "            x_batch[i, :] = x_i\n",
    "            y_batch[i] = self.y[end - 1]\n",
    "            \n",
    "        x_batch = (x_batch - self.x_mean)/self.x_std\n",
    "\n",
    "        return np.expand_dims(x_batch, axis=2), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] [5.62928104]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold, TimeSeriesSplit\n",
    "\n",
    "wav = df_wav['acoustic_data'].values\n",
    "ttf = df_wav['time_to_failure'].values\n",
    "wav_mean = df_wav['acoustic_data'].mean()\n",
    "wav_std = df_wav['acoustic_data'].std()\n",
    "\n",
    "\n",
    "model = Keras1DCnnRegressor(**param['algorithm']['init'])\n",
    "\n",
    "folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "splits = list(folds.split(df_X_train, df_X_train['label']))\n",
    "for n_fold, (train_index, valid_index) in enumerate(splits):\n",
    "    train_gen = Generator(x=wav, y=ttf, x_mean=wav_mean, x_std=wav_std, start_indexes=df_X_train['index'].values[train_index], ts_length=150000, batch_size=128, steps_per_epoch=1, shaking=True)\n",
    "#     valid_gen = Generator(x=wav, y=ttf, x_mean=wav_mean, x_std=wav_std, start_indexes=df_X_train['index'].values[valid_index], ts_length=150000, batch_size=128, steps_per_epoch=100, shaking=False)\n",
    "\n",
    "model.fit_generator(train_gen, epochs=1, eval_set=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_shape = (-1,1)\n",
    "path_param={\n",
    "    'timesteps':150_000, \n",
    "    'input_dim':1, \n",
    "    'cnn_layer_sizes':[64,128,128], \n",
    "    'cnn_kernel_size':10, \n",
    "    'cnn_strides':10, \n",
    "    'cnn_activation':'relu',\n",
    "    'fc_layer_sizes':[1024,1024,16],\n",
    "    'fc_activation':'relu', \n",
    "    'bilstm_layer_sizes':[64],\n",
    "    'dropout':.3,\n",
    "}\n",
    "base_save_dir = create_path('Keras1DCnnRegressor', path_param)\n",
    "param={\n",
    "    'algorithm': {\n",
    "        'cls': 'Keras1DCnnRegressor',\n",
    "        'fit': {\n",
    "            'verbose':1, \n",
    "            'epochs':50, \n",
    "        },\n",
    "        'init': {\n",
    "            'batch':128, \n",
    "            'solver':'adam', \n",
    "            'metric':'mean_absolute_error', \n",
    "            'lr':.0001, \n",
    "            'sgd_momentum':.9, \n",
    "            'sgd_decay':0.0001,\n",
    "            'base_save_dir':base_save_dir, \n",
    "            'alias':'1dcnn_wav',\n",
    "            **path_param\n",
    "        }\n",
    "    },\n",
    "    'columns': ['X'],\n",
    "    'kfold': {\n",
    "        'n_splits': 3,\n",
    "        'random_state': 1985,\n",
    "        'shuffle': True,\n",
    "        'type': 'group'#stratified\n",
    "    },\n",
    "    'scaler': {\n",
    "        'cls': 'ReshapeStandardScaler',\n",
    "        'init':{\n",
    "            'shape':scaler_shape,\n",
    "            'mean':np.array([wav_mean]),\n",
    "            'std':np.array([wav_std]),\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-62bf61f06d64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# run one try\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmytrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_his\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdf_feature_importances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_valid_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test_pred\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mEP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_X_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmytrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmytrial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/wangzhaoxu/ep/LANLEarthquakePrediction2019/common.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(df_train, param, df_test, trial, remark, is_output_feature_importance)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mscaler_cls\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                 \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mX_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/wangzhaoxu/ep/LANLEarthquakePrediction2019/models.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0moriginal_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run one try\n",
    "mytrial = []\n",
    "df_his,  df_feature_importances, df_valid_pred, df_test_pred =  EP.process(df_X_train, param, df_test = df_X_test, trial=mytrial)\n",
    "db.insert(mytrial[0])\n",
    "df_trial = db.select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial['kfold-type'] = df_trial['param'].apply(lambda x: x['kfold']['type'])\n",
    "df_trial['algorithm-init'] = df_trial['param'].apply(lambda x: x['algorithm']['init'])\n",
    "df_trial[['datetime','nfeatures', 'kfold-type', 'algorithm-init', 'train_mae','train_mae_var','val_mae','val_mae_var','mae_diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_his_list = [pd.read_csv(param['algorithm']['init']['base_save_dir'] + '/{}_{}_train_his.csv'.format(param['algorithm']['init']['alias'], i), index_col=0) for i in range(param['kfold']['n_splits'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_lineplot(df_his_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_gpu_p36)",
   "language": "python",
   "name": "conda_tensorflow_gpu_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
