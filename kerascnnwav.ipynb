{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold, TimeSeriesSplit\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional, CuDNNGRU, CuDNNLSTM, RepeatVector, RepeatVector, concatenate,ConvLSTM2D\n",
    "from keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Flatten, Convolution1D,TimeDistributed,Lambda, Activation, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.engine.topology import Layer\n",
    "from keras.initializers import Ones, Zeros\n",
    "\n",
    "from tsfresh.examples import load_robot_execution_failures\n",
    "from tsfresh import extract_features, select_features\n",
    "import optuna\n",
    "\n",
    "from common import EP\n",
    "from models import *\n",
    "from dfdb import DFDB\n",
    "\n",
    "import types\n",
    "import os\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\";  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "   div#notebook-container    { width: 95%; }\n",
       "   div#menubar-container     { width: 65%; }\n",
       "   div#maintoolbar-container { width: 99%; }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "   div#notebook-container    { width: 95%; }\n",
    "   div#menubar-container     { width: 65%; }\n",
    "   div#maintoolbar-container { width: 99%; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_lineplot(dflist, plot_features=None, n_col=3):\n",
    "\n",
    "    n_chart = len(dflist)\n",
    "    n_row = int(n_chart/n_col) if n_chart % n_col == 0 else n_row+1\n",
    "        \n",
    "    fig = plt.figure(figsize=(5*n_col, 3*n_row))\n",
    "    for i, df in enumerate(dflist):\n",
    "        ax = fig.add_subplot(n_row, n_col, i+1)\n",
    "        if type(plot_features) == type(None):\n",
    "            plot_features = df.columns.tolist()\n",
    "        for feat in plot_features:\n",
    "            sns.lineplot(x=df.index, y=df[feat], ax=ax)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train = pd.read_pickle('../feats/sample.pkl')\n",
    "df_wav = pd.read_csv('../data/train.csv', dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32})\n",
    "wav = df_wav['acoustic_data'].values\n",
    "ttf = df_wav['time_to_failure'].values\n",
    "wav_mean = df_wav['acoustic_data'].mean()\n",
    "wav_std = df_wav['acoustic_data'].std()\n",
    "wav = (wav-wav_mean)/wav_std\n",
    "df_X_train['X'] = df_X_train['index'].apply(lambda x: np.expand_dims(wav[x:x+150_000], 1))\n",
    "del df_wav\n",
    "del wav\n",
    "del ttf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train['label'] = df_X_train['y'].apply(lambda x:  int(x) if x<15 else 15)\n",
    "group = df_X_train['season'].values\n",
    "group[np.where(group==17)[0]] = 1\n",
    "df_X_train['group'] = group\n",
    "df_X_train = df_X_train.drop(columns=['season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = '../data/test/'\n",
    "X_test = []\n",
    "for f in sorted(os.listdir(test_file_path)):\n",
    "    df_test_i = pd.read_csv(test_file_path+f)\n",
    "    X_test.append({'index':f.replace('.csv',''), 'X':(df_test_i['acoustic_data'].values-wav_mean)/wav_std})\n",
    "df_X_test = pd.DataFrame(X_test)\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = DFDB('../trial2/kerascnnwav.pkl', auto_commit=False)\n",
    "db.select().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     from dataloader import TokenList, pad_to_longest\n",
    "#     # for transformer\n",
    "# except: pass\n",
    "\n",
    "# class LayerNormalization(Layer):\n",
    "#     def __init__(self, eps=1e-6, **kwargs):\n",
    "#         self.eps = eps\n",
    "#         super(LayerNormalization, self).__init__(**kwargs)\n",
    "#     def build(self, input_shape):\n",
    "#         self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "#                                      initializer=Ones(), trainable=True)\n",
    "#         self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "#                                     initializer=Zeros(), trainable=True)\n",
    "#         super(LayerNormalization, self).build(input_shape)\n",
    "#     def call(self, x):\n",
    "#         mean = K.mean(x, axis=-1, keepdims=True)\n",
    "#         std = K.std(x, axis=-1, keepdims=True)\n",
    "#         return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         return input_shape\n",
    "\n",
    "# class ScaledDotProductAttention():\n",
    "#     def __init__(self, d_model, attn_dropout=0.1):\n",
    "#         self.temper = np.sqrt(d_model)\n",
    "#         self.dropout = Dropout(attn_dropout)\n",
    "#     def __call__(self, q, k, v, mask):\n",
    "#         attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
    "#         if mask is not None:\n",
    "#             mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\n",
    "#             attn = Add()([attn, mmask])\n",
    "#         attn = Activation('softmax')(attn)\n",
    "#         attn = self.dropout(attn)\n",
    "#         output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n",
    "#         return output, attn\n",
    "\n",
    "# class MultiHeadAttention():\n",
    "#     # mode 0 - big martixes, faster; mode 1 - more clear implementation\n",
    "#     def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n",
    "#         self.mode = mode\n",
    "#         self.n_head = n_head\n",
    "#         self.d_k = d_k\n",
    "#         self.d_v = d_v\n",
    "#         self.dropout = dropout\n",
    "#         if mode == 0:\n",
    "#             self.qs_layer = Dense(n_head*d_k, use_bias=False)\n",
    "#             self.ks_layer = Dense(n_head*d_k, use_bias=False)\n",
    "#             self.vs_layer = Dense(n_head*d_v, use_bias=False)\n",
    "#         elif mode == 1:\n",
    "#             self.qs_layers = []\n",
    "#             self.ks_layers = []\n",
    "#             self.vs_layers = []\n",
    "#             for _ in range(n_head):\n",
    "#                 self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "#                 self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "#                 self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n",
    "#         self.attention = ScaledDotProductAttention(d_model)\n",
    "#         self.layer_norm = LayerNormalization() if use_norm else None\n",
    "#         self.w_o = TimeDistributed(Dense(d_model))\n",
    "\n",
    "#     def __call__(self, q, k, v, mask=None):\n",
    "#         d_k, d_v = self.d_k, self.d_v\n",
    "#         n_head = self.n_head\n",
    "\n",
    "#         if self.mode == 0:\n",
    "#             qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n",
    "#             ks = self.ks_layer(k)\n",
    "#             vs = self.vs_layer(v)\n",
    "\n",
    "#             def reshape1(x):\n",
    "#                 s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n",
    "#                 x = tf.reshape(x, [s[0], s[1], n_head, d_k])\n",
    "#                 x = tf.transpose(x, [2, 0, 1, 3])  \n",
    "#                 x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]\n",
    "#                 return x\n",
    "#             qs = Lambda(reshape1)(qs)\n",
    "#             ks = Lambda(reshape1)(ks)\n",
    "#             vs = Lambda(reshape1)(vs)\n",
    "\n",
    "#             if mask is not None:\n",
    "#                 mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\n",
    "#             head, attn = self.attention(qs, ks, vs, mask=mask)  \n",
    "                \n",
    "#             def reshape2(x):\n",
    "#                 s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n",
    "#                 x = tf.reshape(x, [n_head, -1, s[1], s[2]]) \n",
    "#                 x = tf.transpose(x, [1, 2, 0, 3])\n",
    "#                 x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]\n",
    "#                 return x\n",
    "#             head = Lambda(reshape2)(head)\n",
    "#         elif self.mode == 1:\n",
    "#             heads = []; attns = []\n",
    "#             for i in range(n_head):\n",
    "#                 qs = self.qs_layers[i](q)   \n",
    "#                 ks = self.ks_layers[i](k) \n",
    "#                 vs = self.vs_layers[i](v) \n",
    "#                 head, attn = self.attention(qs, ks, vs, mask)\n",
    "#                 heads.append(head); attns.append(attn)\n",
    "#             head = Concatenate()(heads) if n_head > 1 else heads[0]\n",
    "#             attn = Concatenate()(attns) if n_head > 1 else attns[0]\n",
    "\n",
    "#         outputs = self.w_o(head)\n",
    "#         outputs = Dropout(self.dropout)(outputs)\n",
    "#         if not self.layer_norm: return outputs, attn\n",
    "#         # outputs = Add()([outputs, q]) # sl: fix\n",
    "#         return self.layer_norm(outputs), attn\n",
    "    \n",
    "# class ReshapeStandardScaler(object):\n",
    "    \n",
    "#     def  __init__(self, shape, mean, std):\n",
    "        \n",
    "#         assert shape[-1] == len(std.shape), 'the shape is not matched'\n",
    "#         assert shape[-1] == len(mean.shape), 'the shape is not matched'\n",
    "#         self.shape = shape\n",
    "#         self.std = std\n",
    "#         self.mean = mean\n",
    "#         return\n",
    "    \n",
    "#     def fit(self, *args, **kwargs):\n",
    "#         return\n",
    "    \n",
    "#     def transform(self, X):\n",
    "#         original_shape = X.shape\n",
    "#         X = (X.reshape(self.shape) - self.mean)/self.std\n",
    "#         return X.reshape(original_shape)\n",
    "    \n",
    "# def create_path(base_dir, param):\n",
    "#     if base_dir == None:\n",
    "#         return None\n",
    "#     fold_path = base_dir + '/' + ','.join(\"{!s}={!r}\".format(key,val) for (key,val) in param.items())\n",
    "#     if not os.path.exists(fold_path):\n",
    "#         os.makedirs(fold_path)\n",
    "#     return fold_path\n",
    "\n",
    "# class ReshapeStandardScaler(object):\n",
    "    \n",
    "#     def  __init__(self, shape, mean, std):\n",
    "        \n",
    "#         assert shape[-1] == len(std.shape), 'the shape is not matched'\n",
    "#         assert shape[-1] == len(mean.shape), 'the shape is not matched'\n",
    "#         self.shape = shape\n",
    "#         self.std = std\n",
    "#         self.mean = mean\n",
    "#         return\n",
    "    \n",
    "#     def fit(self, **kwargs):\n",
    "#         return\n",
    "    \n",
    "#     def transform(self, X):\n",
    "#         original_shape = X.shape\n",
    "#         X = (X.reshape(self.shape) - self.mean)/self.std\n",
    "#         return X.reshape(original_shape)\n",
    "\n",
    "# class Keras1DCnnRegressor(object):\n",
    "    \n",
    "#     def __init__(self, batch, timesteps, input_dim, cnn_layer_sizes, cnn_kernel_size, cnn_strides, cnn_activation, \n",
    "#                     fc_layer_sizes, fc_activation, dropout, solver, metric, lr, sgd_momentum, sgd_decay, base_save_dir, alias, \n",
    "#                  attention_n_head=5, attention_d_model=256, attention_d_k=64, attention_d_v=64, bilstm_layer_sizes=[]):\n",
    "        \n",
    "#         self.batch = batch\n",
    "#         self.timesteps = timesteps\n",
    "#         self.input_dim = input_dim\n",
    "#         self.cnn_layer_sizes = cnn_layer_sizes\n",
    "#         self.cnn_kernel_size = cnn_kernel_size\n",
    "#         self.cnn_strides = cnn_strides\n",
    "#         self.cnn_activation = cnn_activation\n",
    "#         self.fc_layer_sizes = fc_layer_sizes\n",
    "#         self.fc_activation = fc_activation\n",
    "#         self.dropout = dropout\n",
    "#         self.solver = solver\n",
    "#         self.metric = metric\n",
    "#         self.lr = lr\n",
    "#         self.sgd_momentum = sgd_momentum\n",
    "#         self.sgd_decay = sgd_decay\n",
    "        \n",
    "#         self.regressor = self.build_graph(timesteps, input_dim, cnn_layer_sizes, cnn_kernel_size, cnn_strides, cnn_activation, \n",
    "#                     fc_layer_sizes, fc_activation, attention_n_head, attention_d_model, attention_d_k, attention_d_v, bilstm_layer_sizes, dropout)\n",
    "#         self.compile_graph(self.regressor, solver, metric, lr, sgd_momentum, sgd_decay)\n",
    "        \n",
    "#         self.alias = alias\n",
    "#         self.base_save_dir = base_save_dir\n",
    "#         if (self.alias==None) & (self.base_save_dir==None):\n",
    "#             self.chkpt = None\n",
    "#         else:\n",
    "#             self.chkpt = os.path.join(base_save_dir,'{}.hdf5'.format(alias))\n",
    "\n",
    "#         return\n",
    "    \n",
    "#     def build_graph(self, timesteps, input_dim, cnn_layer_sizes, cnn_kernel_size, cnn_strides, cnn_activation, \n",
    "#                     fc_layer_sizes, fc_activation, attention_n_head, attention_d_model, attention_d_k, attention_d_v, bilstm_layer_sizes, dropout):\n",
    "        \n",
    "#         i = Input(shape = (timesteps, input_dim))\n",
    "#         x = Convolution1D( cnn_layer_sizes[0], kernel_size = cnn_kernel_size, strides = cnn_strides, activation=cnn_activation)(i)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Dropout(dropout)(x)\n",
    "#         for units in cnn_layer_sizes[1:]:\n",
    "#             x = Convolution1D(units, kernel_size = cnn_kernel_size, strides = cnn_strides, activation=cnn_activation)(x)\n",
    "#             x = BatchNormalization()(x)\n",
    "#             x = Dropout(dropout)(x)\n",
    "#         for units in bilstm_layer_sizes:\n",
    "#             x = Bidirectional(CuDNNLSTM(units, return_sequences=True))(x)\n",
    "#         x, slf_attn = MultiHeadAttention(n_head=attention_n_head, d_model=attention_d_model, d_k=attention_d_k, d_v=attention_d_v, dropout=dropout)(x, x, x)\n",
    "#         avg_pool = GlobalAveragePooling1D()(x)\n",
    "#         max_pool = GlobalMaxPooling1D()(x)\n",
    "#         x = concatenate([avg_pool, max_pool])\n",
    "#         for units in fc_layer_sizes[:-1]:\n",
    "#             x = Dense(units, activation=fc_activation)(x)\n",
    "#             x = BatchNormalization()(x)\n",
    "#             x = Dropout(dropout)(x)\n",
    "#         x = Dense(fc_layer_sizes[-1], activation=fc_activation)(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         y = Dense(1)(x)\n",
    "#         regressor = Model(inputs = [i], outputs = [y])\n",
    "#         return regressor\n",
    "    \n",
    "#     def compile_graph(self, model, solver, metric, lr, momentum, decay):\n",
    "#         if solver=='adam':\n",
    "#             optimizer = optimizers.adam(lr=lr)\n",
    "#         elif solver=='sgd':\n",
    "#             optimizer = optimizers.SGD(lr=lr, decay=decay, momentum=momentum, nesterov=True)\n",
    "#         model.compile(optimizer=optimizer, loss=metric)\n",
    "#         return\n",
    "    \n",
    "#     def fit_generator(self, train_gen, eval_set, verbose=1, epochs=200):\n",
    "        \n",
    "        \n",
    "#         df_train_his = pd.DataFrame()\n",
    "# #         prev_val_loss = 999999\n",
    "#         for i in np.arange(epochs):\n",
    "#             if type(eval_set)==type(None):\n",
    "#                 validation_data = None\n",
    "#             else:\n",
    "#                 validation_data = eval_set[0]\n",
    "#             his_train = self.regressor.fit_generator( generator =  train_gen,  epochs = 1,  verbose = 0,  validation_data = validation_data, callbacks = [])\n",
    "#             df_train_his_i = pd.DataFrame(his_train.history)\n",
    "#             df_train_his_i['epochs'] = i\n",
    "#             df_train_his = pd.concat([df_train_his, df_train_his_i], axis=0)\n",
    "            \n",
    "#             if verbose > 0:\n",
    "#                 if validation_data == None:\n",
    "#                     print(df_train_his_i.epochs.values, df_train_his_i.loss.values)\n",
    "#                 else:\n",
    "#                     print(df_train_his_i.epochs.values, df_train_his_i.loss.values, df_train_his_i.val_loss.values)\n",
    "                \n",
    "# #             if (df_train_his_i.val_loss.values[0] < prev_val_loss) & (self.chkpt!=None) :\n",
    "# #                 prev_val_loss = df_train_his_i.val_loss.values[0]\n",
    "# #                 self.regressor.save_weights(self.chkpt)\n",
    "        \n",
    "#         df_train_his.to_csv(self.base_save_dir + '/train_his.csv', index=True)\n",
    "#         return\n",
    "    \n",
    "#     def fit(self, X_train, y_train, eval_set, verbose=1, epochs=200):\n",
    "              \n",
    "#         df_train_his = pd.DataFrame()\n",
    "# #         prev_val_loss = 999999\n",
    "#         for i in np.arange(epochs):\n",
    "#             if type(eval_set)==type(None):\n",
    "#                 validation_data = None\n",
    "#             else:\n",
    "#                 validation_data = eval_set[0]\n",
    "#                 assert type(eval_set[0])==tuple, 'validation_data[0] is not a tuple'\n",
    "#             his_train = self.regressor.fit( X_train, y_train, epochs = 1,  verbose = 0,  batch_size = self.batch,  validation_data = validation_data,  callbacks = [])\n",
    "#             df_train_his_i = pd.DataFrame(his_train.history)\n",
    "#             df_train_his_i['epochs'] = i\n",
    "#             df_train_his = pd.concat([df_train_his, df_train_his_i], axis=0)\n",
    "            \n",
    "#             if verbose > 0:\n",
    "#                 if validation_data == None:\n",
    "#                     print(df_train_his_i.epochs.values, df_train_his_i.loss.values)\n",
    "#                 else:\n",
    "#                     print(df_train_his_i.epochs.values, df_train_his_i.loss.values, df_train_his_i.val_loss.values)\n",
    "                \n",
    "# #             if (df_train_his_i.val_loss.values[0] < prev_val_loss) & (self.chkpt!=None) :\n",
    "# #                 prev_val_loss = df_train_his_i.val_loss.values[0]\n",
    "# #                 self.regressor.save_weights(self.chkpt)\n",
    "                \n",
    "#         df_train_his.to_csv(self.base_save_dir + '/train_his.csv', index=True)\n",
    "            \n",
    "#         return df_train_his\n",
    "    \n",
    "#     def predict(self, X):\n",
    "#         return self.regressor.predict(X)[:,0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(keras.utils.Sequence):\n",
    "\n",
    "#     def __init__(self, x, y, x_mean, x_std, start_indexes, ts_length, batch_size, steps_per_epoch, shaking=True, shuffle=True):\n",
    "#         self.x = x\n",
    "#         self.y = y\n",
    "#         self.start_indexes = start_indexes\n",
    "#         self.ts_length = ts_length\n",
    "#         self.batch_size = batch_size\n",
    "#         self.steps_per_epoch = steps_per_epoch\n",
    "#         self.x_mean = x_mean\n",
    "#         self.x_std = x_std\n",
    "#         self.shaking = shaking\n",
    "#         self.shuffle = shuffle\n",
    "#         self.point = 0\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return self.steps_per_epoch\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "        \n",
    "#         if self.shuffle:\n",
    "#             start_indexes_epoch = np.random.choice(self.start_indexes, size=self.batch_size)\n",
    "#         else:\n",
    "#             start_indexes_epoch = self.start_indexes[self.point:self.point+self.batch_size]\n",
    "#             self.point += self.batch_size\n",
    "#             if self.point > len(self.start_indexes) - self.batch_size:\n",
    "#                 self.point = 0\n",
    "            \n",
    "#         if self.shaking:\n",
    "#             shifts = np.random.randint(0, int(self.ts_length*.2), size=self.batch_size) - int(self.ts_length*.1)\n",
    "#         else:\n",
    "#             shifts = np.zeros(self.batch_size)\n",
    "            \n",
    "#         x_batch = np.empty((self.batch_size, self.ts_length))\n",
    "#         y_batch = np.empty(self.batch_size, )\n",
    "\n",
    "#         for i, start_idx in enumerate(start_indexes_epoch):\n",
    "#             end = start_idx + shifts[i] + self.ts_length\n",
    "#             if end < self.ts_length:\n",
    "#                 end = self.ts_length\n",
    "#             if end >= self.x.shape[0]:\n",
    "#                 end = self.x.shape[0]\n",
    "#             x_i = self.x[int(end-self.ts_length):int(end)]\n",
    "#             x_batch[i, :] = x_i\n",
    "#             y_batch[i] = self.y[int(end - 1)]\n",
    "            \n",
    "#         x_batch = (x_batch - self.x_mean)/self.x_std\n",
    "\n",
    "#         return np.expand_dims(x_batch, axis=2), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def processG(df_index, x, y, param, df_test=None, trial=None, remark=None):\n",
    "\n",
    "#         assert 'y' in df_index.columns.tolist(), 'y is not in df_index'\n",
    "#         assert 'index' in df_index.columns.tolist(), 'index is not in df_index'\n",
    "#         assert 'index' not in param['columns'], 'index is in features'\n",
    "#         assert 'y' not in param['columns'], 'y is in features'\n",
    "#         assert 'label' not in param['columns'], 'label is in features'\n",
    "#         assert 'group' not in param['columns'], 'group is in features'\n",
    "#         assert EP.check_param(param), 'param format is not right '\n",
    "#         assert (type(trial) == list) | (trial == None), 'trial is neither list nor none'\n",
    "\n",
    "\n",
    "#         df_test_pred = None\n",
    "#         if type(df_test) == pd.DataFrame:\n",
    "#             assert 'index' in df_test.columns.tolist(), 'index is not in df_test'\n",
    "#             df_test_pred = pd.concat([df_test_pred, df_test[['index']]], axis=1)\n",
    "\n",
    "#         history = []\n",
    "#         df_valid_pred = pd.DataFrame()\n",
    "\n",
    "#         # stratified,group,timeseries\n",
    "#         if 'splits' in param['kfold']:\n",
    "#             splits = param['kfold']['splits']\n",
    "#         else:\n",
    "#             if param['kfold']['type'] == 'stratified':\n",
    "#                 assert 'label' in df_index.columns.tolist(), 'label is not in df_index'\n",
    "#                 folds = StratifiedKFold(n_splits=param['kfold']['n_splits'], shuffle=param['kfold']['shuffle'],\n",
    "#                                         random_state=param['kfold']['random_state'])\n",
    "#                 splits = list(folds.split(df_index, df_index['label']))\n",
    "#             elif param['kfold']['type'] == 'group':\n",
    "#                 assert 'group' in df_index.columns.tolist(), 'group is not in df_index'\n",
    "#                 folds = GroupKFold(n_splits=param['kfold']['n_splits'])\n",
    "#                 splits = list(folds.split(df_index, groups=df_index['group']))\n",
    "#             elif param['kfold']['type'] == 'timeseries':\n",
    "#                 folds = TimeSeriesSplit(n_splits=param['kfold']['n_splits'])\n",
    "#                 splits = list(folds.split(df_index))\n",
    "#             else:\n",
    "#                 folds = KFold(n_splits=param['kfold']['n_splits'], shuffle=param['kfold']['shuffle'],\n",
    "#                               random_state=param['kfold']['random_state'])\n",
    "#                 splits = list(folds.split(df_index))\n",
    "\n",
    "\n",
    "#         regressor_cls = EP.str2class(param['algorithm']['cls'])\n",
    "        \n",
    "#         x_mean = np.mean(x)\n",
    "#         x_std = np.std(x)\n",
    "#         epochs = param['algorithm']['fit']['epochs']\n",
    "#         batch_size = param['algorithm']['init']['batch']\n",
    "        \n",
    "#         for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "            \n",
    "#             start_indexes_train = df_index['index'].values[train_index]\n",
    "#             start_indexes_valid = df_index['index'].values[valid_index]\n",
    "#             train_gen = Generator(x=x, y=y, x_mean=x_mean, x_std=x_std, start_indexes=start_indexes_train, ts_length=150000, batch_size=batch_size, steps_per_epoch=epochs)\n",
    "#             valid_gen = Generator(x=x, y=y, x_mean=x_mean, x_std=x_std, start_indexes=start_indexes_valid, ts_length=150000, batch_size=len(start_indexes_valid), steps_per_epoch=1, shaking=False, shuffle=False)\n",
    "            \n",
    "#             fit_param = param['algorithm']['fit'].copy()\n",
    "#             if 'eval_set' in fit_param:\n",
    "#                 fit_param['eval_set'] = [valid_gen]\n",
    "            \n",
    "#             algorithm_init_param = param['algorithm']['init'].copy()\n",
    "#             if 'alias' in list(algorithm_init_param.keys()):\n",
    "#                 algorithm_init_param['alias'] = algorithm_init_param['alias'] + '_{}'.format(fold_n)\n",
    "#             model = regressor_cls(**algorithm_init_param)\n",
    "            \n",
    "#             model.fit_generator(train_gen, **fit_param)\n",
    "#             y_valid_pred = model.predict_generator(valid_gen)\n",
    "#             y_train_pred = model.predict_generator(train_gen)\n",
    "\n",
    "#             original_index = df_index['index'].values[valid_index]\n",
    "#             df_valid_pred_i = pd.DataFrame({'index': original_index, 'predict': y_valid_pred, 'fold_n': np.zeros(y_valid_pred.shape[0]) + fold_n})\n",
    "#             df_valid_pred = pd.concat([df_valid_pred, df_valid_pred_i], axis=0)\n",
    "\n",
    "#             if type(df_test) == pd.DataFrame:\n",
    "                \n",
    "#                 X_test = np.array(df_test['X'].values.tolist())\n",
    "#                 X_test =  (X_test - x_mean)/x_std\n",
    "#                 y_test_pred = model.predict(X_test)\n",
    "#                 df_test_pred_i = pd.DataFrame({fold_n: y_test_pred})\n",
    "#                 df_test_pred = pd.concat([df_test_pred, df_test_pred_i], axis=1)\n",
    "\n",
    "#             history.append({'fold_n': fold_n, 'train': mean_absolute_error(y_train, y_train_pred), 'valid': mean_absolute_error(y_valid, y_valid_pred)})\n",
    "\n",
    "#         df_his = pd.DataFrame(history)\n",
    "#         df_valid_pred = df_valid_pred.sort_values(by=['index'])\n",
    "#         df_valid_pred = df_valid_pred.reset_index(drop=True)\n",
    "\n",
    "#         if type(df_test) == pd.DataFrame:\n",
    "#             df_test_pred = df_test_pred.sort_values(by=['index'])\n",
    "#             df_test_pred = df_test_pred.reset_index(drop=True)\n",
    "\n",
    "#         if type(trial) == list:\n",
    "#             pid_ = os.getpid()\n",
    "#             datetime_ = datetime.datetime.now()\n",
    "#             connection_file = os.path.basename(kernel.get_connection_file())\n",
    "#             val_mae_mean = np.mean(df_his.valid)\n",
    "#             val_mae_var = np.var(df_his.valid)\n",
    "#             train_mae_mean = np.mean(df_his.train)\n",
    "#             train_mae_var = np.var(df_his.train)\n",
    "\n",
    "#             trial.append({'datetime': datetime_, 'kernel': connection_file, 'remark': remark, 'val_mae': val_mae_mean,\n",
    "#                           'train_mae': train_mae_mean, 'val_mae_var': val_mae_var, 'train_mae_var': train_mae_var,\n",
    "#                           'mae_diff': val_mae_mean - train_mae_mean,\n",
    "#                           'df_his': df_his, \n",
    "#                           'df_valid_pred': df_valid_pred, 'df_test_pred': df_test_pred, 'param': param.copy(),\n",
    "#                           'nfeatures': len(columns)})\n",
    "\n",
    "#         return df_his, _, df_valid_pred, df_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold, TimeSeriesSplit\n",
    "\n",
    "# wav = df_wav['acoustic_data'].values\n",
    "# ttf = df_wav['time_to_failure'].values\n",
    "# wav_mean = df_wav['acoustic_data'].mean()\n",
    "# wav_std = df_wav['acoustic_data'].std()\n",
    "\n",
    "# model = Keras1DCnnRegressor(**param['algorithm']['init'])\n",
    "\n",
    "# folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "# splits = list(folds.split(df_X_train, df_X_train['label']))\n",
    "# for n_fold, (train_index, valid_index) in enumerate(splits):\n",
    "#     train_gen = Generator(x=wav, y=ttf, x_mean=wav_mean, x_std=wav_std, start_indexes=df_X_train['index'].values[train_index], ts_length=150000, batch_size=128, steps_per_epoch=1, shaking=True)\n",
    "# #     valid_gen = Generator(x=wav, y=ttf, x_mean=wav_mean, x_std=wav_std, start_indexes=df_X_train['index'].values[valid_index], ts_length=150000, batch_size=128, steps_per_epoch=100, shaking=False)\n",
    "\n",
    "# model.fit_generator(train_gen, epochs=1, eval_set=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_shape = (-1,1)\n",
    "path_param={\n",
    "    'timesteps':150_000, \n",
    "    'input_dim':1, \n",
    "    'cnn_layer_sizes':[16,64], \n",
    "    'cnn_kernel_size':10, \n",
    "    'cnn_strides':10, \n",
    "    'cnn_activation':'relu',\n",
    "    'fc_layer_sizes':[1024,16],\n",
    "    'fc_activation':'relu', \n",
    "    'bilstm_layer_sizes':[],\n",
    "    'dropout':.3,\n",
    "}\n",
    "base_save_dir = create_path('Keras1DCnnRegressor', path_param)\n",
    "param={\n",
    "    'algorithm': {\n",
    "        'cls': 'Keras1DCnnRegressor',\n",
    "        'fit': {\n",
    "            'verbose':1, \n",
    "            'epochs':50, \n",
    "            'eval_set':()\n",
    "        },\n",
    "        'init': {\n",
    "            'batch':16, \n",
    "            'solver':'adam', \n",
    "            'metric':'mean_absolute_error', \n",
    "            'lr':.0001, \n",
    "            'sgd_momentum':.9, \n",
    "            'sgd_decay':0.0001,\n",
    "            'base_save_dir':base_save_dir, \n",
    "            'alias':'1dcnn_wav',\n",
    "            **path_param\n",
    "        }\n",
    "    },\n",
    "    'columns': ['X'],\n",
    "    'kfold': {\n",
    "        'n_splits': 3,\n",
    "        'random_state': 1985,\n",
    "        'shuffle': True,\n",
    "        'type': 'group'#stratified\n",
    "    },\n",
    "    'scaler': None\n",
    "#     {\n",
    "#         'cls': None,\n",
    "#         'init':{\n",
    "#             'shape':scaler_shape,\n",
    "#             'mean':np.array([wav_mean]),\n",
    "#             'std':np.array([wav_std]),\n",
    "#         }\n",
    "#     }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mytrial = []\n",
    "# df_his,  df_feature_importances, df_valid_pred, df_test_pred =  processG(df_X_train, wav, ttf, param, df_test = df_X_test, trial=mytrial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] [5.25204812] [7.97254419]\n",
      "[1] [4.05684487] [4.68224865]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-62bf61f06d64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# run one try\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmytrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_his\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdf_feature_importances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_valid_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test_pred\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mEP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_X_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmytrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmytrial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/wangzhaoxu/ep/LANLEarthquakePrediction2019/common.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(df_train, param, df_test, trial, remark, is_output_feature_importance)\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mX_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0my_valid_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/wangzhaoxu/ep/LANLEarthquakePrediction2019/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, eval_set, verbose, epochs)\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validation_data[0] is not a tuple'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0mhis_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m             \u001b[0mdf_train_his_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhis_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mdf_train_his_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run one try\n",
    "mytrial = []\n",
    "df_his,  df_feature_importances, df_valid_pred, df_test_pred =  EP.process(df_X_train, param, df_test = df_X_test, trial=mytrial)\n",
    "db.insert(mytrial[0])\n",
    "df_trial = db.select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial['kfold-type'] = df_trial['param'].apply(lambda x: x['kfold']['type'])\n",
    "df_trial['algorithm-init'] = df_trial['param'].apply(lambda x: x['algorithm']['init'])\n",
    "df_trial[['datetime','nfeatures', 'kfold-type', 'algorithm-init', 'train_mae','train_mae_var','val_mae','val_mae_var','mae_diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_his_list = [pd.read_csv(param['algorithm']['init']['base_save_dir'] + '/{}_{}_train_his.csv'.format(param['algorithm']['init']['alias'], i), index_col=0) for i in range(param['kfold']['n_splits'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_lineplot(df_his_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_gpu_p36)",
   "language": "python",
   "name": "conda_tensorflow_gpu_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
